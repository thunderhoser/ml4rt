#!/bin/tcsh

#SBATCH --job-name="evaluate_by_albedo"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --time=24:00:00
#SBATCH --array=1-60
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=evaluate_by_time_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_standalone/ml4rt"
set TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_models/experiment08"

set DENSE_LAYER_COUNTS=(3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3 3 2 3)
set DENSE_LAYER_DROPOUT_RATES=(0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000 0.100 0.100 0.000)
set L2_WEIGHTS=(0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162 0.0000003162 0.0000010000 0.0000003162)
set ALBEDO_BIN_STRINGS=("000" "000" "000" "001" "001" "001" "002" "002" "002" "003" "003" "003" "004" "004" "004" "005" "005" "005" "006" "006" "006" "007" "007" "007" "008" "008" "008" "009" "009" "009" "010" "010" "010" "011" "011" "011" "012" "012" "012" "013" "013" "013" "014" "014" "014" "015" "015" "015" "016" "016" "016" "017" "017" "017" "018" "018" "018" "019" "019" "019")

set dense_layer_count=${DENSE_LAYER_COUNTS[$SLURM_ARRAY_TASK_ID]}
set dense_layer_dropout_rate=${DENSE_LAYER_DROPOUT_RATES[$SLURM_ARRAY_TASK_ID]}
set l2_weight=${L2_WEIGHTS[$SLURM_ARRAY_TASK_ID]}
set albedo_bin_string=${ALBEDO_BIN_STRINGS[$SLURM_ARRAY_TASK_ID]}

set dense_layer_count_string=`printf "%d" $dense_layer_count`
set dense_layer_dropout_string=`printf "%.3f" $dense_layer_dropout_rate`
set l2_weight_string=`printf "%.10f" $l2_weight`

set top_model_dir_name="${TOP_MODEL_DIR_NAME}/num-dense-layers=${dense_layer_count_string}_dense-dropout=${dense_layer_dropout_string}_l2-weight=${l2_weight_string}"

set model_file_names={${top_model_dir_name}/model*.h5}
set model_file_name=${model_file_names[$#model_file_names]}
set model_dir_name=`echo $model_file_name | rev | cut -c 4- | rev`

set prediction_dir_name="${model_dir_name}/isotonic_regression/validation/by_time"

python3 -u "${CODE_DIR_NAME}/evaluate_neural_net.py" \
--input_prediction_file_name="${prediction_dir_name}/predictions_albedo-bin=${albedo_bin_string}.nc" \
--num_bootstrap_reps=100 \
--output_dir_name="${prediction_dir_name}"

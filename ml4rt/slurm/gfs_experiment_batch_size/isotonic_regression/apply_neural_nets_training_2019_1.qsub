#!/bin/tcsh

#SBATCH --job-name="apply_neural_nets_training_2019"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
##SBATCH --nodes=1
##SBATCH --ntasks=1
##SBATCH --cpus-per-task=1
##SBATCH --ntasks-per-node=1
#SBATCH --time=02:00:00
#SBATCH --array=1
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=apply_neural_nets_training_2019_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_standalone/ml4rt"
set TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_models/gfs_experiment_batch_size"
set ORIG_EXAMPLE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_project/gfs_data/examples/new_heights/normalized/subset/training"
set NEW_EXAMPLE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_project/gfs_data/examples/orig_heights/subset/training"
set NEW_NORM_FILE_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_project/gfs_data/examples/orig_heights/training/learning_examples_for_norm.nc"

set FIRST_TRAINING_TIME_STRING="2019-12-01-000000"
set LAST_TRAINING_TIME_STRING="2019-12-31-235959"

set EXAMPLES_PER_BATCH_COUNTS=(64 91 128 181 256 362 512 64 91 128 181 256 362 512 64 91 128 181 256 362 512 64 91 128 181 256 362 512 64 91 128 181 256 362 512)
set BATCHES_PER_EPOCH_COUNTS=(100 100 100 100 100 100 100 250 250 250 250 250 250 250 500 500 500 500 500 500 500 750 750 750 750 750 750 750 1000 1000 1000 1000 1000 1000 1000)

set num_examples_per_batch=${EXAMPLES_PER_BATCH_COUNTS[$SLURM_ARRAY_TASK_ID]}
set num_batches_per_epoch=${BATCHES_PER_EPOCH_COUNTS[$SLURM_ARRAY_TASK_ID]}
set num_examples_per_batch_string=`printf "%03d" $num_examples_per_batch`
set num_batches_per_epoch_string=`printf "%04d" $num_batches_per_epoch`

set top_model_dir_name="${TOP_MODEL_DIR_NAME}/num-examples-per-batch=${num_examples_per_batch_string}_num-batches-per-epoch=${num_batches_per_epoch_string}"

set model_file_names={${top_model_dir_name}/model*.h5}
set model_file_name=${model_file_names[$#model_file_names]}
set model_dir_name=`echo $model_file_name | rev | cut -c 4- | rev`
set prediction_file_name="${model_dir_name}/isotonic_regression/training/orig_predictions.nc"

python3 -u "${CODE_DIR_NAME}/apply_neural_net_with_interp.py" \
--input_model_file_name="${model_file_name}" \
--input_orig_example_dir_name="${ORIG_EXAMPLE_DIR_NAME}" \
--input_new_example_dir_name="${NEW_EXAMPLE_DIR_NAME}" \
--input_new_norm_file_name="${NEW_NORM_FILE_NAME}" \
--half_window_size_for_interp_px=3 \
--first_time_string="${FIRST_TRAINING_TIME_STRING}" \
--last_time_string="${LAST_TRAINING_TIME_STRING}" \
--output_file_name="${prediction_file_name}"

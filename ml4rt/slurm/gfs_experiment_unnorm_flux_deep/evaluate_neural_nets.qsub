#!/bin/tcsh

#SBATCH --job-name="evaluate_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --time=30:00:00
#SBATCH --array=1-50
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=evaluate_neural_nets_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_standalone/ml4rt"
set TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4rt_models/gfs_experiment_unnorm_flux_deep"

set FLUX_LOSS_SCALING_FACTORS=(0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00)
set flux_loss_scaling_factor=${FLUX_LOSS_SCALING_FACTORS[$SLURM_ARRAY_TASK_ID]}
set flux_loss_scaling_factor=`printf "%.2f" $flux_loss_scaling_factor`

set top_model_dir_name="${TOP_MODEL_DIR_NAME}/flux-loss-scaling-factor=${flux_loss_scaling_factor}"
set model_file_names={${top_model_dir_name}/model*.h5}

set i=1

while ($i <= ${#model_file_names})
    set model_file_name=${model_file_names[$i]}
    set model_dir_name=`echo $model_file_name | rev | cut -c 4- | rev`
    set prediction_file_name="${model_dir_name}/validation/predictions.nc"
    set validation_dir_name="${model_dir_name}/validation"
    
    python3 -u "${CODE_DIR_NAME}/evaluate_neural_net.py" \
    --input_prediction_file_name="${prediction_file_name}" \
    --num_bootstrap_reps=1 \
    --output_dir_name="${validation_dir_name}"
    
    @ i = $i + 2
end
